{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c25ac9f",
   "metadata": {},
   "source": [
    "# 1. Download the pretrained model and upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b8eaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Transformers\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 20.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Transformers) (21.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Transformers) (0.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Transformers) (5.4.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 100 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Transformers) (2021.4.4)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 66.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Transformers) (4.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Transformers) (4.61.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Transformers) (2.26.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 63.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->Transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->Transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->Transformers) (3.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->Transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->Transformers) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->Transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->Transformers) (2021.10.8)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->Transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->Transformers) (8.0.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->Transformers) (1.16.0)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, Transformers\n",
      "Successfully installed Transformers-4.12.5 huggingface-hub-0.2.1 sacremoses-0.0.46 tokenizers-0.10.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d939aa6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9cd811709946fb9ab060d2eb0a8be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507dd070035f43628509cac0f88f9061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "pretrained_model=\"bert-base-uncased\"\n",
    "model = BertForSequenceClassification.from_pretrained(pretrained_model)\n",
    "model.save_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3694e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method save_pretrained in module transformers.modeling_utils:\n",
      "\n",
      "save_pretrained(save_directory:Union[str, os.PathLike], save_config:bool=True, state_dict:Union[dict, NoneType]=None, save_function:Callable=<function save at 0x7f6ec9cf1b70>, push_to_hub:bool=False, **kwargs) method of transformers.models.bert.modeling_bert.BertForSequenceClassification instance\n",
      "    Save a model and its configuration file to a directory, so that it can be re-loaded using the\n",
      "    `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n",
      "    \n",
      "    Arguments:\n",
      "        save_directory (:obj:`str` or :obj:`os.PathLike`):\n",
      "            Directory to which to save. Will be created if it doesn't exist.\n",
      "        save_config (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "            Whether or not to save the config of the model. Useful when in distributed training like TPUs and need\n",
      "            to call this function on all processes. In this case, set :obj:`save_config=True` only on the main\n",
      "            process to avoid race conditions.\n",
      "        state_dict (nested dictionary of :obj:`torch.Tensor`):\n",
      "            The state dictionary of the model to save. Will default to :obj:`self.state_dict()`, but can be used to\n",
      "            only save parts of the model or if special precautions need to be taken when recovering the state\n",
      "            dictionary of a model (like when using model parallelism).\n",
      "        save_function (:obj:`Callable`):\n",
      "            The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n",
      "            need to replace :obj:`torch.save` by another method.\n",
      "        push_to_hub (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether or not to push your model to the Hugging Face model hub after saving it.\n",
      "    \n",
      "            .. warning::\n",
      "    \n",
      "                Using :obj:`push_to_hub=True` will synchronize the repository you are pushing to with\n",
      "                :obj:`save_directory`, which requires :obj:`save_directory` to be a local clone of the repo you are\n",
      "                pushing to if it's an existing folder. Pass along :obj:`temp_dir=True` to use a temporary directory\n",
      "                instead.\n",
      "    \n",
      "        kwargs:\n",
      "            Additional key word arguments passed along to the\n",
      "            :meth:`~transformers.file_utils.PushToHubMixin.push_to_hub` method.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.save_pretrained)\n",
    "# dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eacd8fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 418M\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  678 Dec  6 13:43 config.json\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 418M Dec  6 13:43 pytorch_model.bin\n",
      "config.json\n",
      "pytorch_model.bin\n",
      "total 36\n",
      "drwx------ 2 root     root     16384 Nov 29 03:17 lost+found\n",
      "drwxrwxr-x 6 ec2-user ec2-user  4096 Nov 29 08:41 sagemaker_train_demo\n",
      "drwxrwxr-x 4 ec2-user ec2-user  4096 Nov 29 08:42 docker_test_folder\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  1140 Nov 30 04:00 Untitled.ipynb\n",
      "drwxrwxr-x 8 ec2-user ec2-user  4096 Dec  1 07:44 sagemaker-pytorch-inference-toolkit\n",
      "drwxrwxr-x 7 ec2-user ec2-user  4096 Dec  6 13:44 amazon-sagemaker-bert-pytorch\n"
     ]
    }
   ],
   "source": [
    "!ls -rtlh ./model/\n",
    "!cd model && tar czvf ../model.tar.gz *\n",
    "!cd ../ && ls -lrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426d1b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/amazon-sagemaker-bert-pytorch\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07093819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-833719260605\n",
      "arn:aws:iam::833719260605:role/ivy-sagemaker-role\n",
      "s3://sagemaker-us-east-1-833719260605/sagemaker/ivy-pretrained-pytorch-bert/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/ivy-pretrained-pytorch-bert\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(bucket)\n",
    "print(role)\n",
    "\n",
    "fObj = open(\"model.tar.gz\", \"rb\")\n",
    "key = os.path.join(prefix, \"model.tar.gz\")\n",
    "# boto3.Session().resource(\"s3\").Bucket(bucket).Object(key).upload_fileobj(fObj)\n",
    "s3_pretrain_model = \"s3://{}\".format(os.path.join(bucket, key))\n",
    "print(s3_pretrain_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8903180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-833719260605/grammer-classification-bert-base-uncase-2021-12-02-10-33-48-251/model.tar.gz to ./trained_model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://sagemaker-us-east-1-833719260605/grammer-classification-bert-base-uncase-2021-12-02-10-33-48-251/model.tar.gz trained_model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b10fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-06 13:45:46  405255270 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://sagemaker-us-east-1-833719260605/sagemaker/ivy-pretrained-pytorch-bert/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe2b48",
   "metadata": {},
   "source": [
    "# 2. Prepare the Inference Script\n",
    "To deploy a pretrained PyTorch model, we will need to use the PyTorch estimator object to create a PyTorchModel object and set a different entry_point.\n",
    "\n",
    "An implementation of model_fn is required for inference script. We are going to use default implementations of input_fn, predict_fn, output_fn and model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896d01ee",
   "metadata": {},
   "source": [
    "# 3. Deploy With the pretrained model and inference script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32c373",
   "metadata": {},
   "source": [
    "## 3.1 Create Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5902c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=s3_pretrain_model,\n",
    "    role=role,\n",
    "    entry_point='deploy_ei.py',\n",
    "    source_dir='code',\n",
    "    framework_version='1.3.1',\n",
    "    py_version='py3',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c3bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91cea6ec",
   "metadata": {},
   "source": [
    "## 3.2 Create inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "963c7b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name = 'grammar-classification-{}-ep'.format(time.time()).replace('.', '').replace('_', '')\n",
    "instance_type = \"ml.t2.medium\" #'ml.m5.large'\n",
    "# Function will exit before endpoint is finished creating\n",
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "#     accelerator_type=accelerator_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    wait=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2004a860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [0]\n"
     ]
    }
   ],
   "source": [
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "\n",
    "res = predictor.predict('Please remember to delete me when you are done.')\n",
    "print(\"Predicted class:\", np.argmax(res, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627db4f",
   "metadata": {},
   "source": [
    "# 4. Test the endpoint\n",
    "the code can run outside sagemaker, like lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8000f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sagemaker_runtime_client = boto3.Session().client(service_name='runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f4fe1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def predict_grammer_text(text):\n",
    "    response = sagemaker_runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        ContentType='application/json', \n",
    "        Body=json.dumps(text)\n",
    "    )\n",
    "    result = response['Body'].read()\n",
    "    result = json.loads(result)\n",
    "    print('Probabilities for all classes: ', result)\n",
    "    predicted_class = np.argmax(result)\n",
    "    if predicted_class == 0:\n",
    "        print('Grammer incorrect!')\n",
    "    else:\n",
    "        print('Grammer correct.')\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ea8172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for all classes:  [[0.517944872379303, 0.37516987323760986]]\n",
      "Grammer incorrect!\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "s = \"I am a girl come from Chinese.\"\n",
    "x = predict_grammer_text(s)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8294c7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for all classes:  [[0.5746420621871948, 0.3318248689174652]]\n",
      "Grammer incorrect!\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "s = \"I am a girl from China.\"\n",
    "x = predict_grammer_text(s)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee5ac7",
   "metadata": {},
   "source": [
    "# 5. Clean the endpoint\n",
    "<b>Please remember to clean the endpoint if it is just for practice</b><br/>\n",
    "the endpoint will create ec2 instance in the backend to serve the predict request. if we didn't clean the endpoint, the ec2 instance will keep running in the backend which will generate cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4980b35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grammar-classification-16388043152128417-ep\n",
      "<class 'NoneType'> None\n"
     ]
    }
   ],
   "source": [
    "print(predictor.endpoint_name)\n",
    "resp = sagemaker_session.delete_endpoint(endpoint_name=predictor.endpoint_name)\n",
    "print(type(resp), resp)\n",
    "\n",
    "resp = sagemaker_session.delete_endpoint_config(endpoint_config_name=predictor.endpoint_name)\n",
    "print(type(resp), resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96347b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-inference-2021-12-06-15-25-44-762\n",
      "<class 'NoneType'> None\n"
     ]
    }
   ],
   "source": [
    "print(pytorch_model.name)\n",
    "resp = sagemaker_session.delete_model(model_name=pytorch_model.name)\n",
    "print(type(resp), resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
